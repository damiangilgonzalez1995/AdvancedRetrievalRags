{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_KEY')\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectordb = Chroma(persist_directory=\"./jonhWick_db\", embedding_function=embeddings, collection_name=\"doc_jonhWick\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MY_ENV_VAR = os.getenv('MY_ENV_VAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tecnica intenta evitar el sobre consumo de tokens, y por lo tanto, dar demsiado contexto al LLM. Para la creación de un RAG, siempre hay que tener en mente dos puntos:\n",
    "\n",
    " - Context Windows: Cuanto más documentos obtengamos de la vectore store, más información tendrá el LLM para dar una buena respuesta.\n",
    " - Recall: Cuanto más documentos sean recuperados de la vectore store, la probabilidad de obtener chunks irrevalantes es mayor y por lo tanto, el recall disminuye.\n",
    "\n",
    "Parece que no hay solución para este problema. Cuando aumentamos una de las métricas, la otra parece estar destinada a disminuir. ¿Estamos seguros de eso?\n",
    "\n",
    "Es aqui cuando se presenta esta tecnica, compression retriever, focalizandonos en la técnica del reranking. Digamos que esta técnica consta de dos pasos bien diferenciados:\n",
    "\n",
    " - Obtener una buena cantidad de docs relevantes en función del input/pregunta. Normlamente fijamos los K más relevantes.\n",
    " - Recalcular cuales de estos documentos son realmente relevantes.\n",
    "\n",
    "\n",
    "Para el primer paso, se usa lo que se conoce como Bi-encoder, que no es nada más que lo que solemos usar para hacer un RAG básico. Vectorizar nuestros documentos. vectorizar la quuery y calcular la similitud con cualquier métrica de nuestra elección.\n",
    "\n",
    "El segundo paso es algo diferente a lo que estamos habituados a ver. Este recalculo/reranking, aparecen los reranking model o cross-encoder. Estos modelos esperan como input dos documentos/textos devolviendo una score de similitud entre el par. \n",
    "\n",
    "Si uno de estos dos inputs es la query y el otro es un chunk, podemos calcular la similitud entre ambos. Pero Damián, eso ya lo hacemos cuando aplicamos los bi-encoders.\n",
    "\n",
    "Esto es totalmente cierto, pero hay una caracteristica clave. Y es que este tipo de técnica mejora el resultado de la similitud entre dos textos.\n",
    "\n",
    "De acuerdo, funciona mejor, entonces porque no lo usamos directamente con todos los chunks, en vez de solo con los K mejores. Porque sería terriblemente costoso en tiempo y en dinero/computación. Por ello hacemos un primer filtro de los chunks más cercanos en similitud con la query, reduciendo el uso del modelo de reranking a tan solo K veces. \n",
    "\n",
    "De hecho este tipo de modelos recogen los dos inputs comentados previamente, y devuelve tan solo un numero entre 0 y 1. De hecho, para ser más especificos, esto se conoce como modelos \"Cross-Encoder\". Estos modelos no devuelven un sentences embedding, por lo tanto su uso está muy restringido a unos casos de uso.\n",
    "\n",
    "\n",
    "Para entender mejor la acquitectura de este tipo de modelos, veamos un ejemplo visual.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Volviendo de nuevo al coste computacional y de tiempo, si se aplicase directamente los cross-encoders, pensar que con cada nueva consulta, se debería de calcular la similitud de la query con cada uno de los documentos. Algo que no es nada optimo.\n",
    "\n",
    "En cambio usando los Bi-encoder, la representación vectorial de los documentos es la misma para cada nueva consulta. \n",
    "\n",
    "Tenemos entonces un metodo bastante superior que es costoso de ejecutar, y por otro lado, otro metodo que funciona bien pero que no tiene un gran costo computacional con cada nueva consulta. Todo esto finaliza con la conclusión de unificar estos dos métodos para un mejor RAG. Y esto es conocido como los contextual Compression con el método de reranking. \n",
    "\n",
    "\n",
    "https://www.sbert.net/examples/applications/cross-encoder/README.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create a compression retriever, we need a base retriever first. In this case I will use the naive retriever (the simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_retriever = vectordb.as_retriever(search_kwargs={ \"k\" : 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to do a Naive RAG.\n",
    "\n",
    "## Remember:\n",
    "\n",
    "- R -> Retrieval\n",
    "- A -> Augmented\n",
    "- G -> Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualCompressionRetriever(base_compressor=CohereRerank(client=<cohere.client.Client object at 0x00000170F1131C30>, top_n=3, model='rerank-english-v2.0', cohere_api_key=None, user_agent='langchain'), base_retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000170DD6F7DC0>, search_kwargs={'k': 10}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have already created the retriever object\n",
    "compression_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"\\\n",
    "You are happy assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we are going to create a Rag Parent doc Retrieval. For that, we are going to use LCEL (LangChain Expression Language)\n",
    "If you want to learn more about LCEL, check this good tutorial: https://www.youtube.com/watch?v=O0dUOtOIrfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, people generally liked John Wick.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "setup_and_retrieval = RunnableParallel({\"question\": RunnablePassthrough(), \"context\": compression_retriever })\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "compressor_retrieval_chain = setup_and_retrieval | rag_prompt | chat_model | output_parser\n",
    "\n",
    "\n",
    "compressor_retrieval_chain.invoke( \"Did people generally like John Wick?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't know the answer.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressor_retrieval_chain.invoke(\"What are the reviews with a score greater than 7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the reviews with a score greater than 7 and say bad things about the movie?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the reviews with a score greater than 7 and say bad things about the movie?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context> > 3:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the reviews with a score greater than 7 and say bad things about the movie?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context> > 3:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the reviews with a score greater than 7 and say bad things about the movie?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context>] [819ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are happy assistant. Use the context provided below to answer the question.\\n\\nIf you do not know the answer, or are unsure, say you don't know.\\n\\nQuery:\\nWhat are the reviews with a score greater than 7 and say bad things about the movie?\\n\\nContext:\\n[Document(page_content=\\\": 8\\\\nReview: It's hard to find anything bad to say about John Wick. The action is beautifully choreographed, the setup is surprisingly emotional for an action flick, and Keanu.... What more is there to say? If you love action or even just like it you will be in for the ride of your life.\\\", metadata={'Author': 'MrHeraclius', 'Movie_Title': 'John Wick 1', 'Rating': 5, 'Review_Date': '23 February 2020', 'Review_Title': ' love this movie highly recommend\\\\n', 'Review_Url': '/review/rw5503708/?ref_=tt_urv', 'row': 8, 'source': 'data/john_wick_1.csv', 'relevance_score': 0.86153823}), Document(page_content=\\\": 1\\\\nReview: It looks as if the filmmakers realized that the public was sick of certain movies. Most movies nowadays are full of wire work, sci-fi, etc. often topped with icing made of needless or predictable background stories that add nothing. For example the beautiful girlfriend who gives the moral speeches or the troubled teenage daughter, who aren't present here.\\\", metadata={'Author': 'CountJonnie', 'Movie_Title': 'John Wick 1', 'Rating': 7, 'Review_Date': '17 January 2015', 'Review_Title': ' Story: 3 minutes; Entertainment: 101 minutes.\\\\n', 'Review_Url': '/review/rw3164002/?ref_=tt_urv', 'row': 1, 'source': 'data/john_wick_1.csv', 'relevance_score': 0.51013047}), Document(page_content=\\\": 18\\\\nReview: And all of this equals boredom. 2 hours of the same thing over and over. A movie for young teens who judge movies as they do video games; the more kills, the better. After the 30th guy got his head blown off, it became totally boring to me. John Wick gets hit by a car 6 times, gets thrown down a flight of stairs 10 times, gets punched and kicked 500 times...and yet he keeps fighting. He fights 30 guys at the same time, with a gun or without, and he always wins. There is zero plot, zero character development, zero drama...it's all mindless action. And repetitive. Fistfights, guns, and cars. Nothing else. Nothing creative. Well they tried to be creative a couple times...like wasting 15 minutes with a woman doing an elaborate yet gross suicide...and for no reason other than to slap something 'original' up on the screen. It was completely pointless. How this movie is rated highly is beyond me. Like I say, it must be young teens who judge movies on number of kills. PS - I'm not exaggerating. The main bad guy simply puts a message into his phone, and within 3 minutes, literally the entire city is out to kill John Wick. So he has to fight 30 guys at a time, killing 500 guys within 20 minutes or so. Mind-numbingly stupid and boring.\\\", metadata={'Author': 'jmichael3387', 'Movie_Title': 'John Wick 2', 'Rating': 2, 'Review_Date': '28 February 2017', 'Review_Title': ' Mindless repetitive action...no story, no drama, he can kill 100 guys at the same time...\\\\n', 'Review_Url': '/review/rw3650338/?ref_=tt_urv', 'row': 18, 'source': 'data/john_wick_2.csv', 'relevance_score': 0.4966622})]\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:ChatOpenAI] [799ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I don't know.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I don't know.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"name\": null,\n",
      "            \"id\": \"run-f89880e5-dbbe-45ef-8b41-34e7f791ce8d-0\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 5,\n",
      "      \"prompt_tokens\": 839,\n",
      "      \"total_tokens\": 844\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_b28b39ffa8\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"I don't know.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.62s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"I don't know.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "compressor_retrieval_chain.invoke(\"What are the reviews with a score greater than 7 and say bad things about the movie?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finaly, the query is : talk bad about the movie and the filter is \"Rating\" greater than 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
