{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "  loader = CSVLoader(\n",
    "    encoding=\"utf8\",\n",
    "    file_path=f\"data/john_wick_{i}.csv\",\n",
    "    metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
    "  )\n",
    "\n",
    "  movie_docs = loader.load()\n",
    "  for doc in movie_docs:\n",
    "\n",
    "    # We add metadate about the number of the movi\n",
    "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
    "\n",
    "  documents.extend(movie_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de retriaver se usa cuando el input que proporciona el usuario está compuesto por un número reducido de palabras o la precisión de la consulta debe de ser impecable. Veamos un ejemplo para entenderlo mejor:\n",
    "\n",
    "Imagina que hemos creado un RAG para para reconocer posibles enfermedades sabiendo alguno de sus sintomas. En el caso que tengamos un Naive RAG, posiblemente recolectemos una serie de posibles enfermedades que solo coincidan en uno o dos sintimas, dejando a nuestra herramienta un poco en mal lugar. \n",
    "\n",
    "Este es un caso ideal para usar Parent Doc Retriever. Y es que tipo de tecnica consiste en trocear grandes chunks (parent chunk) en aun trozos más pequeños (child chunk). Al tener unos chunks con reducido tamaño, hace que la información que contienen esté más concentrada y por lo tanto, su valor informativo no se diluya entre parrafos de texto.\n",
    "\n",
    "Pero se sabe que el contexto es importante para una buena respuesta en lenguaje natural. Por lo tanto, en vez de devolver los child chunk, se obtiene los parent chunks a los que pertenece los child chink que más similitud tienen con la consulta del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "parent_docs = documents\n",
    "\n",
    "# Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "# Splitters\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# We don't need a parent splitter because the data cames from CSV file, and each row is a parent doc.\n",
    "\n",
    "\n",
    "# Stores\n",
    "store = InMemoryStore()\n",
    "vectorstore = Chroma(embedding_function=embeddings, collection_name=\"fullDoc\", persist_directory=\"./JohnWick_db_parentsRD\")\n",
    "\n",
    "\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    # parent_splitter =parent_splitter\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parent chunks  is: 75\n",
      "Number of child chunks is: 3701\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parent chunks  is: {len(list(store.yield_keys()))}\")\n",
    "\n",
    "print(f\"Number of child chunks is: {len(parent_document_retriever.vectorstore.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retriever.vectorstore.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to do a Naive RAG.\n",
    "\n",
    "## Remember:\n",
    "\n",
    "- R -> Retrieval\n",
    "- A -> Augmented\n",
    "- G -> Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParentDocumentRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002BD7AC879A0>, docstore=<langchain.storage.in_memory.InMemoryBaseStore object at 0x000002BD7AC86A40>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000002BD7AC87610>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have already created the retriever object\n",
    "parent_document_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"\\\n",
    "You are happy assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we are going to create a Rag Parent doc Retrieval. For that, we are going to use LCEL (LangChain Expression Language)\n",
    "If you want to learn more about LCEL, check this good tutorial: https://www.youtube.com/watch?v=O0dUOtOIrfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, people generally like John Wick.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "setup_and_retrieval = RunnableParallel({\"question\": RunnablePassthrough(), \"context\": parent_document_retriever })\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "parent_retrieval_chain = setup_and_retrieval | rag_prompt | chat_model | output_parser\n",
    "\n",
    "\n",
    "parent_retrieval_chain.invoke(\"Did people generally like John Wick?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Did people generally like John Wick?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Did people generally like John Wick?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context> > 3:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Did people generally like John Wick?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context> > 3:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Did people generally like John Wick?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<question,context>] [233ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did people generally like John Wick?\",\n",
      "  \"context\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did people generally like John Wick?\",\n",
      "  \"context\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are happy assistant. Use the context provided below to answer the question.\\n\\nIf you do not know the answer, or are unsure, say you don't know.\\n\\nQuery:\\nDid people generally like John Wick?\\n\\nContext:\\n[]\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:llm:ChatOpenAI] [689ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, people generally like John Wick.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, people generally like John Wick.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"name\": null,\n",
      "            \"id\": \"run-d024fdd3-1c01-49d6-9222-5ebe1321314f-0\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 8,\n",
      "      \"prompt_tokens\": 52,\n",
      "      \"total_tokens\": 60\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_c2295e73ad\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, people generally like John Wick.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [925ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Yes, people generally like John Wick.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, people generally like John Wick.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "parent_retrieval_chain.invoke(\"Did people generally like John Wick?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
